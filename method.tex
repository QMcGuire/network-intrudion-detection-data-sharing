\section{Methodology}
\label{sec:method}
\begin{method}
In this section, we will describe how we curate the data-sets. We will also detail our ML pipeline to detect anomalies.
\subsection{Data Pipeline}
\myparab{CIC-DS 2017 \& 2018.} The data is the processed data that resulted from the network traffic analysis of the raw pcap files using CICFlowMeter. CICFlowMeter takes the raw packet data in pcap files  and separates them into flows, or sequences of packets carrying information from a source computer to a destination (\textit{e.g.} another host, a multicast group or a broadcast domains). After that, it will analyse these flows and return the characteristics (features) of these flows. In total, CICFlowmeter returned a dataset that has 80 features.

Next, we need to label these flows as benign or malicous (if they are malicious, we label the attack types) based on the attacker IP, victim IP, and timing of the flow. If the flow matches with the first two features and the timing of the flow overlaps with any of the attack types, we will label that flow to be the corresponding attack type. 

\myparab{IoT data.}This data was given to us as the raw pcap files, and as such, we need to use a network traffic analysis tool to extract features of these files. In this case, we choose to use NFStream, a popular network flow aggregation and statistical features extraction tool which is widely used in network traffic analysis. After that, we compared these flows against log files provided by the IoT dataset and labeled these flows based on source IP, source port, destination IP, destination port and timing of the flows. If the flow matches with the first four features and the timing of the flow overlaps with when the attack occurs, the flow will be labeled according to that attack.

\subsection{ML Pipeline}
\myparab{Feature Selection.} In order to train a good model, we first exclude some features we deem inappropriate and select important network features to be used in our machine learning model. We remove features which might affect the performance of our model or are too complicated to include in such as IP address and port number. This is because IP address of benign and of attackers can vary significantly. As such, including them might not be very useful. On the other hand, should these features be included, they should be treated as discreet values. Given that there are many different IP and port number, the number of features might explode. As such, we decide to use these following 9 features to characterize the networks.


\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|}
    \hline
        Feature name & Type \\ \hline
        bidirectional\_duration\_ms & continuous \\ \hline
        biddirectional\_packets & continuous \\ \hline
        bidirectional\_bytes & continuous \\ \hline
        src2dst\_duration\_ms & continuous \\ \hline
        src2dst\_packets & continuous \\ \hline
        src2dst\_bytes & continuous \\ \hline
        dst2src\_duration\_ms & continuous \\ \hline
        dst2src\_packets & continuous \\ \hline
        dst2src\_bytes & continuous \\ \hline
    \end{tabular}
\end{table}


\myparab{Creating training and testing subsets.}
Since we are focusing on unsupervised learning (as we want to see how well data sharing can help in detecting unknown attacks), we used benign data as training data. We conducted several tests to answer the previously defined questions.

\textbf{Does data sharing has the potential to improve the performance of NIDS?} 
To answer this question, we tried using or augmenting another dataset's trainset helps to build a better NIDS. We built models using 3 training scenarios. The first one is trained using only the benign data of 2017. The second one is trained using only the benign data of 2018. The third one is trained using augmented data from both full 2017 train dataset and full 2018 train datasets. We then tested these models against the 2017 test dataset and 2018 test datasets.

\textbf{Does the amount of data shared matter?} 
In order to evaluate this, we randomly sampled 10 percent of the whole training dataset, used this to train the model, and evaluated the performance of the NIDS to see whether it was impacted by the amount of training data. We also conducted another scenario where, instead of augmenting training datasets according their original size, we sampled them to make the size of each dataset equal. %We used these model to test similarly to the previous questions.

\textbf{What data should be shared?} 
In order to answer this, we used the IoT dataset instead, as it provided more data. Our initial dataset used for training is 1-1. After that, we will augmented different datasets (20-1, 21-1, 34-1) with the dataset 1-1 to see if augmenting data improves performance of NIDS under these scenarios.

\myparab{Models} 
We chose to focus on unsupervised machine learning model in this study because we are interested in seeing if augmenting data can help to improve detection of anomalies that could previously go undetected. We initially tried Principal Component Analysis (PCA), Gaussian Mixture Model (GMM) and Kernel Density Estimation (KDE), but ultimately came to the conclusion that only KDE works well. As such, we will only focus on KDE classifier in this study and changed the hyperparameters of this model to obtain the best performing classifier. 

\myparab{Metrics} 
We choose AUC as the metric to measure the performance of our NIDS because it is a robust metric that takes into account both false positive and true positive rates.

\end{method}
